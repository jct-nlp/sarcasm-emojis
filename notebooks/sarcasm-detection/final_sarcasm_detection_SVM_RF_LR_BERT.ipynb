{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmtiTlTxpp43"
      },
      "source": [
        "#Sarcasm Detection (sarcastic or not)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXpRQTt742oU"
      },
      "source": [
        "\n",
        "\n",
        "Download the dataset of sarcastic and non sarcastic tweets with emojis and splitting it into train and test sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlAn9gxdoOPG",
        "outputId": "a2ae620a-ce32-45f6-d16a-2907836d6da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X1jZgrRaVH-h0xX3k8cSzT0kumIVlmwx\n",
            "To: /content/sarc_emojis_1000.csv\n",
            "100% 1.60M/1.60M [00:00<00:00, 92.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1--LLxkODAryG3TgUHrcggfrYoXnr9UFe\n",
            "To: /content/not_sarc_emojis_1000.csv\n",
            "100% 1.68M/1.68M [00:00<00:00, 43.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown 1X1jZgrRaVH-h0xX3k8cSzT0kumIVlmwx #sarc_emojis_1000.csv\n",
        "! gdown 1--LLxkODAryG3TgUHrcggfrYoXnr9UFe #not_sarc_emojis_1000.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fchnSlyYrq0F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sarcastic_df = pd.read_csv('sarc_emojis_1000.csv')\n",
        "non_sarcastic_df = pd.read_csv('not_sarc_emojis_1000.csv')\n",
        "\n",
        "# Append emoticon to text in both datasets\n",
        "sarcastic_df['text'] = sarcastic_df['text'] + ' ' + sarcastic_df['label']\n",
        "non_sarcastic_df['text'] = non_sarcastic_df['text'] + ' ' + non_sarcastic_df['label']\n",
        "\n",
        "# Add labels to the data\n",
        "sarcastic_df['class'] = 1  # 1 for sarcastic\n",
        "non_sarcastic_df['class'] = 0  # 0 for non sarcastic\n",
        "\n",
        "# Prepare empty dataframes for training and testing sets\n",
        "train_df = pd.DataFrame()\n",
        "test_df = pd.DataFrame()\n",
        "\n",
        "# Assuming 'label' is the column with the label information\n",
        "labels = sarcastic_df['label'].unique()  # or a predefined list of labels if this is not accurate\n",
        "\n",
        "for label in labels:\n",
        "    # Filter sarcastic and non-sarcastic dataframes by label\n",
        "    sarcastic_label_df = sarcastic_df[sarcastic_df['label'] == label]\n",
        "    non_sarcastic_label_df = non_sarcastic_df[non_sarcastic_df['label'] == label]\n",
        "\n",
        "    # Split sarcastic data for the current label into train and test\n",
        "    sarcastic_train = sarcastic_label_df.sample(frac=0.8, random_state=42)\n",
        "    sarcastic_test = sarcastic_label_df.drop(sarcastic_train.index)\n",
        "\n",
        "    # Split non-sarcastic data for the current label into train and test\n",
        "    non_sarcastic_train = non_sarcastic_label_df.sample(frac=0.8, random_state=42)\n",
        "    non_sarcastic_test = non_sarcastic_label_df.drop(non_sarcastic_train.index)\n",
        "\n",
        "    # Append splits to the corresponding training and testing dataframes\n",
        "    train_df = pd.concat([train_df, sarcastic_train, non_sarcastic_train], ignore_index=True)\n",
        "    test_df = pd.concat([test_df, sarcastic_test, non_sarcastic_test], ignore_index=True)\n",
        "\n",
        "# Shuffle the final train and test dataframes to ensure a mix of labels\n",
        "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Extract features and labels for training and testing\n",
        "X_train, y_train = train_df['text'], train_df['class']\n",
        "X_test, y_test = test_df['text'], test_df['class']"
      ],
      "metadata": {
        "id": "OPzcieJ5iL6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ0T7X5h67Jg"
      },
      "outputs": [],
      "source": [
        "def calc_and_print_eveluation_metrics(y_test, y_pred):\n",
        "  print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "  print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
        "  print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
        "  print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeLqmhBqr4rL"
      },
      "source": [
        "##Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrtITKqNr32c"
      },
      "outputs": [],
      "source": [
        "# Create a CountVectorizer to convert text into a bag-of-words representation (unigrams)\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EDlGeDgpOX-"
      },
      "source": [
        "###SVM unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6_A2c1ZpE3q",
        "outputId": "de98a977-37e1-4778-99aa-6f38707389f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM unigrams results:\n",
            "\n",
            "Accuracy: 0.7225\n",
            "Precision: 0.7226113056528264\n",
            "Recall: 0.72225\n",
            "F1 Score: 0.7224306076519129\n"
          ]
        }
      ],
      "source": [
        "# Build an SVM model\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('SVM unigrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDhHfNl4sIEr"
      },
      "source": [
        "###Random Forest unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwB3z5zAsHMb",
        "outputId": "b131f87b-14e7-47c3-c41d-2e9c130e88b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest unigrams results:\n",
            "\n",
            "Accuracy: 0.71725\n",
            "Precision: 0.737431693989071\n",
            "Recall: 0.67475\n",
            "F1 Score: 0.7046997389033942\n"
          ]
        }
      ],
      "source": [
        "# Build a Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('Random Forest unigrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic Regression unigrams"
      ],
      "metadata": {
        "id": "cl4yXkdfHxQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Build a Logistic Regression model\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=100)\n",
        "log_reg_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "print('Logistic Regression unigrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaCU4N7yH1Q4",
        "outputId": "c3f32555-2919-4d75-936f-65541cec6700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression unigrams results:\n",
            "\n",
            "Accuracy: 0.731125\n",
            "Precision: 0.7366265676990018\n",
            "Recall: 0.7195\n",
            "F1 Score: 0.7279625648159859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXUjntRapfkg"
      },
      "source": [
        "##Ngrams - 2 chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12ByDTiosSr9"
      },
      "outputs": [],
      "source": [
        "# Create a CountVectorizer to convert text into a bag-of-words representation (character n-grams)\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkMxklh7sTql"
      },
      "source": [
        "###SVM ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yLZ2Yf6pSe1",
        "outputId": "83b39025-f07d-49de-8d5c-b31613ca5c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM ngrams results:\n",
            "\n",
            "Accuracy: 0.69625\n",
            "Precision: 0.7048538622129437\n",
            "Recall: 0.67525\n",
            "F1 Score: 0.6897344228804902\n"
          ]
        }
      ],
      "source": [
        "# Build an SVM model\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('SVM ngrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dr32pndpnle"
      },
      "source": [
        "###Random Forest ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEjr2DQHpmz1",
        "outputId": "5240d541-dd3f-4426-d5f7-e6017d9c4179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest ngrams results:\n",
            "\n",
            "Accuracy: 0.71175\n",
            "Precision: 0.7195438050803525\n",
            "Recall: 0.694\n",
            "F1 Score: 0.7065411046067702\n"
          ]
        }
      ],
      "source": [
        "# Build a Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('Random Forest ngrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic Regression ngrams"
      ],
      "metadata": {
        "id": "1W-QaW9oJawZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Build a Logistic Regression model\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "print('Logistic Regression n-grams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HizyZ8DVJe8n",
        "outputId": "09c36d28-1815-47ad-e53f-98dc98b9a401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression n-grams results:\n",
            "\n",
            "Accuracy: 0.6935\n",
            "Precision: 0.6976506639427987\n",
            "Recall: 0.683\n",
            "F1 Score: 0.6902475997978778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpZsYmyFkUO7"
      },
      "source": [
        "##Ngrams - 3 chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRXAGkoYkUPI"
      },
      "outputs": [],
      "source": [
        "# Create a CountVectorizer to convert text into a bag-of-words representation (character n-grams)\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "howtiYnAkUPI"
      },
      "source": [
        "###SVM ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9f399e-4e83-4cd4-fc0b-77d5d85bffd6",
        "id": "IbC86jZEkUPJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM ngrams results:\n",
            "\n",
            "Accuracy: 0.727875\n",
            "Precision: 0.7289625722180356\n",
            "Recall: 0.7255\n",
            "F1 Score: 0.7272271645157248\n"
          ]
        }
      ],
      "source": [
        "# Build an SVM model\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('SVM ngrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3-HeRXmkUPJ"
      },
      "source": [
        "###Random Forest ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydvMCt8UkUPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dffc1c-82c0-4132-ac9a-8add0119f808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest ngrams results:\n",
            "\n",
            "Accuracy: 0.736125\n",
            "Precision: 0.7549257759784076\n",
            "Recall: 0.69925\n",
            "F1 Score: 0.7260220635950683\n"
          ]
        }
      ],
      "source": [
        "# Build a Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('Random Forest ngrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic Regression ngrams"
      ],
      "metadata": {
        "id": "vN2-3PYykUPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Build a Logistic Regression model\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "print('Logistic Regression n-grams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "ii1tN9cokUPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "797cedc9-45b0-4bf8-820f-73dbbb2b892d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression n-grams results:\n",
            "\n",
            "Accuracy: 0.741625\n",
            "Precision: 0.7413233458177279\n",
            "Recall: 0.74225\n",
            "F1 Score: 0.741786383510306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kaIlJAM4yczJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS-ESeA7pctF"
      },
      "source": [
        "##Ngrams - 2 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn-lGhHMpctH"
      },
      "outputs": [],
      "source": [
        "# Create a CountVectorizer to convert text into a bag-of-words representation (character n-grams)\n",
        "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSfQtXxxpctI"
      },
      "source": [
        "###SVM ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlWkRFkgpctJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11111c40-9c1d-42a5-e159-31d57bb7a2b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM ngrams results:\n",
            "\n",
            "Accuracy: 0.68625\n",
            "Precision: 0.7143268124280783\n",
            "Recall: 0.62075\n",
            "F1 Score: 0.664258962011771\n"
          ]
        }
      ],
      "source": [
        "# Build an SVM model\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('SVM ngrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBhjGUaqpctK"
      },
      "source": [
        "###Random Forest ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gRJk3HcpctL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41e6ab1-e740-4452-b00d-5653f3fce53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest ngrams results:\n",
            "\n",
            "Accuracy: 0.6865\n",
            "Precision: 0.75\n",
            "Recall: 0.5595\n",
            "F1 Score: 0.6408934707903781\n"
          ]
        }
      ],
      "source": [
        "# Build a Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print eveluation metrics\n",
        "print('Random Forest ngrams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic Regression ngrams"
      ],
      "metadata": {
        "id": "HRxcxSstpctM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Build a Logistic Regression model\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "print('Logistic Regression n-grams results:\\n')\n",
        "calc_and_print_eveluation_metrics(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "f8gtgTkUpctN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43864e54-a833-4c08-ab25-1f10d9426f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression n-grams results:\n",
            "\n",
            "Accuracy: 0.7005\n",
            "Precision: 0.7392601431980907\n",
            "Recall: 0.6195\n",
            "F1 Score: 0.6741022850924918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlhZQyr0yuwF"
      },
      "source": [
        "##Bert"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "labels = df['class'].tolist()\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Assuming binary classification\n",
        "\n",
        "# Tokenize and encode the input texts\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Create TensorDataset for training\n",
        "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define optimizer and training parameters\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 4\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids, attention_mask, label = batch\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_loss}')\n",
        "\n",
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, label = batch\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(label.tolist())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "print('BERT results: \\n')\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "exYSfRTXKrUg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "0e47930e-9737-49d0-d2ea-939c0444b414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8c7d07f59a62>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usH-EygUzaHY"
      },
      "source": [
        "##RoBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tWNemgGzeNJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "labels = df['class'].tolist()\n",
        "\n",
        "# Load pre-trained RoBERTa model and tokenizer\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Assuming binary classification\n",
        "\n",
        "# Tokenize and encode the input texts\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Create TensorDataset for training\n",
        "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Define optimizer and training parameters\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids, attention_mask, label = batch\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Average Training Loss: {avg_loss}')\n",
        "\n",
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, label = batch\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(label.tolist())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BeLqmhBqr4rL",
        "LXUjntRapfkg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}